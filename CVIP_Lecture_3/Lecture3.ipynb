{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge Detection and Image Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start; some clarifications:\n",
    "* Blur kernel - does indeed use center pixel. My bad üñêüèΩ\n",
    "* Histogram equalisation - same pixels land on same value (this is why it works best when applied to images with much higher color depth than palette size, like continuous data or 16-bit gray-scale images e.g. xrays). See example [here](https://en.wikipedia.org/wiki/Histogram_equalization).\n",
    "* In theory, if the histogram equalization function is known, then the original histogram can be recovered.\n",
    "\n",
    "Moving on; today we will cover the following topics in OpenCV\n",
    "\n",
    "* Laplacian edge detection\n",
    "* Sobel edge detection\n",
    "* Canny edge detection\n",
    "* Contour identification\n",
    "* Bounding box creation\n",
    "\n",
    "We'll also touch upon a few other segmentation techniques:\n",
    "* KMeans\n",
    "* Masking\n",
    "\n",
    "Edge detection is needed to detect objects in a picture. An edge is mathematically defined as a distinct change in pixel value. \n",
    "\n",
    "![Pre derivative](images/prederivative.png)\n",
    "Edges can be identified by derivatives\n",
    "\n",
    "The first derivative (or slope) will have the highest value at the point of the edge. Sobel Filters use this concept to identify edges.\n",
    "![First derivative](images/first_derivative.png)\n",
    "\n",
    "The second derivative will be 0 at the edge. Laplacian uses the second derivative to identify the edge.\n",
    "![Second Derivative](images/second_derivative.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in image and save out rgb and grayscale versions\n",
    "image_bgr = cv2.imread(\"images/coin_collection.jpeg\")\n",
    "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "image = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "plt.imshow(image,'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sobel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sobel filters involve a matrix convolution that gives the approximate derivative and therefore gradient. \n",
    "![Sobel Kernel](images/sobel_kernels.png)\n",
    "These filters are applied both horizontally and vertically.\n",
    "\n",
    "We use a 64 bit float representation (as opposed to the 8-bit representation used for images before) because edge definitions can be +ve (black-white) or -ve (white-black). 8-bit representation is therefore no longer sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sobelX = cv2.Sobel(image, cv2.CV_64F, 1, 0) #vertical edges\n",
    "sobelY = cv2.Sobel(image, cv2.CV_64F, 0, 1) #horizontal edges\n",
    "sobelX = np.uint8(np.absolute(sobelX))\n",
    "sobelY = np.uint8(np.absolute(sobelY))\n",
    "sobelCombined = cv2.bitwise_or(sobelX, sobelY)\n",
    "titles = ['Original Image', 'Combined',\n",
    "            'Sobel X', 'Sobel Y']\n",
    "images = [image, sobelCombined, sobelX, sobelY]\n",
    "plt.figure(figsize=[20,7])\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1),plt.imshow(images[i],'gray')\n",
    "    plt.title(titles[i])\n",
    "    plt.xticks([]),plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sobelCombined,'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplacian\n",
    "The Laplacian involves calculating the double derivative of pixel values. In an ideal scenario the edge the first derivative is highest and the second derivative is 0. This is the fundamental idea behind Laplacian edge detection.\n",
    "\n",
    "![Laplace Kernel](images/laplacian_kernel.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lap = cv2.Laplacian(image, cv2.CV_64F)\n",
    "lap = np.uint8(np.absolute(lap))\n",
    "plt.imshow(lap,'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canny Edge Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Canny edge detector is a multi-step process. It involves blurring the image to remove noise, computing Sobel \n",
    "gradient images in the x and y direction, suppressing edges, and finally a hysteresis thresholding stage that \n",
    "determines if a pixel is ‚Äúedge-like‚Äù or not.\n",
    "\n",
    "1. Gaussian blur to remove noise.\n",
    "2. Compute the Sobel gradients along the x (G<sub>x</sub>) and y (G<sub>y</sub>) directions. \n",
    "![Gradient Calculation](images/sobel_x_andy.png)\n",
    "\n",
    "The final gradient and angle are calculated by the formulae below\n",
    "![Gradient Calculation](images/canny_sobel_formula.png)\n",
    "\n",
    "3. After getting gradient magnitude and direction, a full scan of image is done to remove any unwanted pixels which may not constitute the edge. For this, at every pixel, the pixel is checked to see if it is a local maximum in its neighborhood in the direction of gradient.\n",
    "\n",
    "4. Finally, we check which of the edges are *really* edges and which are not. For this, we need two threshold values; minVal and maxVal. Any edges with intensity gradient more than maxVal are sure to be edges (so we keep them) and those below minVal are sure to be non-edges (and therefore discarded). Those that lie in-between these two thresholds are classified as edges or non-edges based on their connectivity. If they are connected to \"sure-edge\" pixels, they are considered to be part of edges. Otherwise, they are also discarded. See the image below:\n",
    "\n",
    "![Gradient Calculation](images/canny_hysteresis.jpeg)\n",
    "The edge A is above the maxVal, so considered as \"sure-edge\". Although edge C is below maxVal, it is connected to edge A, so that also considered as valid edge and we get that full curve. But edge B, although it is above minVal and is in same region as that of edge C, it is not connected to any \"sure-edge\", so that is discarded. So it is very important that we have to select minVal and maxVal accordingly to get the correct result.\n",
    "\n",
    "This stage also removes small pixels noises on the assumption that edges are long lines.\n",
    "\n",
    "OpenCV puts all the above in single function, cv2.Canny(). \n",
    "\n",
    "First argument is our input image. Second and third arguments are our minVal and maxVal respectively.\n",
    "\n",
    "Optionally arguments are:\n",
    "\n",
    "* Aperture_size - size of Sobel kernel used for find image gradients. By default it is 3. \n",
    "* L2gradient - specifies the equation for finding gradient magnitude. If it is True, it uses the equation mentioned above, which is more accurate, otherwise it uses this function for Edge\\_Gradient \\: (G) = |G_x| + |G_y|. By default, it is False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in an image and blur it (this step isn't necessary, but leads to improved results)\n",
    "image = cv2.imread(\"images/coin_collection.jpeg\")\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "blurred = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "interim_image = blurred\n",
    "plt.imshow(blurred,'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further pre-processing the image, can help improve the results tremendously. e.g. using more blur, binarising using thesholds etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresh = cv2.threshold(blurred, 180, 255, cv2.THRESH_BINARY)[1]\n",
    "# interim_image = thresh\n",
    "# plt.imshow(thresh,'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canny = cv2.Canny(interim_image, 30, 150)\n",
    "plt.imshow(canny,'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Contours\n",
    "The first argument to cv2.findContours is our edged image. It‚Äôs important to note that this function is destructive to the image you pass in. If you intend using that image later on in your code, it‚Äôs best to make a copy of it, using the NumPy copy method.\n",
    "\n",
    "The second argument is the type of contours we want. We use cv2.RETR_EXTERNAL to retrieve only the outermost contours (i.e., the contours that follow the outline of the coin). We can also pass in cv2.RETR_LIST to grab all contours. Other methods include hierarchical contours using cv2.RETR_COMP and cv2.RETR_TREE.\n",
    "\n",
    "Our last argument is how we want to approximate the contour. We use cv2.CHAIN_APPROX_SIMPLE to compress horizontal, vertical, and diagonal segments into their end-points only. This saves both computation and memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cnts, _) = cv2.findContours(canny.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "coins = image.copy()\n",
    "cv2.drawContours(coins, cnts, -1, (255, 0, 0), 2)\n",
    "print (f\"We have {len(canny)} edges.\")\n",
    "print (f\"We have {len(cnts)} contours.\")\n",
    "plt.imshow(coins,'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sort the contours by area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contour_areas(cnts):\n",
    "    all_areas= []\n",
    "    for cnt in cnts:\n",
    "        area= cv2.contourArea(cnt)\n",
    "        all_areas.append(area)\n",
    "    return all_areas\n",
    "\n",
    "print (\"Contour Areas before Sorting\", get_contour_areas(cnts))\n",
    "sorted_contours= sorted(cnts, key=cv2.contourArea, reverse= True)\n",
    "print (\"Contour Areas after Sorting\", get_contour_areas(sorted_contours))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post Processing\n",
    "You can add a bounding box around each contour by using the bounding rect function. \n",
    "\n",
    "Let's do so for the 9 largest contours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cnt in sorted_contours[0:9]:\n",
    "    x,y,w,h = cv2.boundingRect(cnt)\n",
    "    cv2.rectangle(coins,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "\n",
    "plt.imshow(coins,'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then use the bounding box to crop out each segment.\n",
    "\n",
    "Alternatively, you can use moments locate the center of the contour, and do some processing based on that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coins = image.copy()\n",
    "# loop over the contours\n",
    "for c in sorted_contours[0:9]:\n",
    "\t# compute the center of the contour\n",
    "\tM = cv2.moments(c)\n",
    "\tcX = int(M[\"m10\"] / M[\"m00\"])\n",
    "\tcY = int(M[\"m01\"] / M[\"m00\"])\n",
    "\t# draw the contour and center of the shape on the image\n",
    "\tcv2.drawContours(coins, [c], -1, (0, 255, 0), 2)\n",
    "\tcv2.circle(coins, (cX, cY), 7, (255, 255, 255), -1)\n",
    "\tcv2.putText(coins, \"center\", (cX - 20, cY - 20),\n",
    "\t\tcv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 255, 255), 2)\n",
    "\t# show the image\n",
    "\tplt.imshow(coins,'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's [a ton of other things](https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_contours/py_table_of_contents_contours/py_table_of_contents_contours.html) you can do with Contours.\n",
    "\n",
    "**References for the above section**\n",
    "1. https://docs.opencv.org/master/da/d22/tutorial_py_canny.html\n",
    "2. https://docs.opencv.org/3.4/d2/d2c/tutorial_sobel_derivatives.html\n",
    "3. https://docs.opencv.org/3.4/d5/db5/tutorial_laplace_operator.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Techniques Usable for Segmentation\n",
    "\n",
    "### KMeans\n",
    "#### Input parameters\n",
    "samples : It should be of np.float32 data type, and each feature should be put in a single column.\n",
    "\n",
    "nclusters(K) : Number of clusters required at end\n",
    "\n",
    "criteria : It is the iteration termination criteria. When this criteria is satisfied, algorithm iteration stops.\n",
    "This should be a tuple of 3 parameters. They are \n",
    "\n",
    "`( type, max_iter, epsilon )`:\n",
    "1. type of termination criteria. It has 3 flags as below:\n",
    "* cv.TERM_CRITERIA_EPS - stop the algorithm iteration if specified accuracy, epsilon, is reached.\n",
    "* cv.TERM_CRITERIA_MAX_ITER - stop the algorithm after the specified number of iterations, max_iter.\n",
    "* cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER - stop the iteration when any of the above condition is met.\n",
    "\n",
    "2. max_iter - An integer specifying maximum number of iterations.\n",
    "3. epsilon - Required accuracy\n",
    "\n",
    "attempts : Flag to specify the number of times the algorithm is executed using different initial labellings. The algorithm returns the labels that yield the best compactness. This compactness is returned as output.\n",
    "\n",
    "flags : This flag is used to specify how initial centers are taken. Normally two flags are used for this : cv.KMEANS_PP_CENTERS and cv.KMEANS_RANDOM_CENTERS.\n",
    "\n",
    "#### Output parameters\n",
    "compactness : It is the sum of squared distance from each point to their corresponding centers.\n",
    "\n",
    "labels : This is the label array to that shows to which cluster each element belongs '0', '1'...\n",
    "\n",
    "centers : This is the array of the centers of clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Image and Convert to RGB\n",
    "image_bgr = cv2.imread(\"images/cartoon_beach.webp\")\n",
    "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Reshape the image into a 2D array of pixels and 3 color values (RGB)\n",
    "plt.imshow(image_rgb)\n",
    "pixel_vals = image_rgb.reshape((-1,3))\n",
    " \n",
    "# Convert to float type\n",
    "pixel_vals = np.float32(pixel_vals)\n",
    "\n",
    "\"\"\"This defines the criteria for the algorithm to stop running.\n",
    "Either 100 iterations are run or the epsilon (which is the required accuracy)\n",
    "becomes 85% and the algorithm stops.\"\"\"\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform k-means clustering with number of clusters defined as 2\n",
    "#Random centres are initially chosen\n",
    "k = 2\n",
    "retval, labels, centers = cv2.kmeans(pixel_vals, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    " \n",
    "# convert data into 8-bit values\n",
    "centers = np.uint8(centers)\n",
    "segmented_data = centers[labels.flatten()]\n",
    " \n",
    "# reshape data into the original image dimensions\n",
    "segmented_image = segmented_data.reshape((image_rgb.shape))\n",
    "plt.imshow(segmented_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform k-means clustering with number of clusters defined as 3\n",
    "#Random centres are initially chosen\n",
    "k = 3\n",
    "retval, labels, centers = cv2.kmeans(pixel_vals, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    " \n",
    "# convert data into 8-bit values\n",
    "centers = np.uint8(centers)\n",
    "segmented_data = centers[labels.flatten()]\n",
    " \n",
    "# reshape data into the original image dimensions\n",
    "segmented_image = segmented_data.reshape((image_rgb.shape))\n",
    "plt.imshow(segmented_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform k-means clustering with number of clusters defined as 4\n",
    "#Random centres are initially chosen\n",
    "k = 4\n",
    "retval, labels, centers = cv2.kmeans(pixel_vals, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    " \n",
    "# convert data into 8-bit values\n",
    "centers = np.uint8(centers)\n",
    "segmented_data = centers[labels.flatten()]\n",
    " \n",
    "# reshape data into the original image dimensions\n",
    "segmented_image = segmented_data.reshape((image_rgb.shape))\n",
    "plt.imshow(segmented_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking\n",
    "We've covered masking before. \n",
    "\n",
    "Only new thing is the inRange function. We use this to only allow values within a certain range (of RGB or HSV) instead of just specifying one intensity value like we do in grayscale.\n",
    "\n",
    "The result is a binarised image with matching values as white, and the rest as black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_hsv = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2HSV)\n",
    "light_blue = (90, 70, 50)\n",
    "dark_blue = (128, 255, 255)\n",
    "# You can use the following values for green\n",
    "# light_green = (40, 40, 40)\n",
    "# dark_greek = (70, 255, 255)\n",
    "mask = cv2.inRange(image_hsv, light_blue, dark_blue)\n",
    "plt.imshow(mask, 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = cv2.bitwise_and(image_rgb, image_rgb, mask=mask)\n",
    "plt.imshow(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CVIP2022",
   "language": "python",
   "name": "cvip2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
